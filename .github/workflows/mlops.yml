
#GitHub Actions MLOps Pipeline for Fraud Detection Service

#Trigger on push to main, manual dispatch, or monthly schedule


name: MLOps Pipeline

on:
  push:
    branches: [main]
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * *'  # # Every day on the first day at midnight UTC


permissions:
  contents: read # allow checking out code
  packages: write # allow pushing Docker images (GHCR)

jobs:
  # ───────────────────────────────────────────────────────────────
  # 1) deploy_job: Extract current DBs from Docker volumes into workspace
  deploy_job:
    runs-on: [ self-hosted, Windows ]
    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Ensure fraud-data volume exists
        shell: pwsh
        run: |
          # Try to inspect; suppress stderr
          docker volume inspect fraud-data 2>$null
          # If last exit code ≠ 0 -> volume doesn't exist -> create it
          if ($LASTEXITCODE -ne 0) {
            docker volume create fraud-data
          }

      - name: Ensure fraud-reference volume exists
        shell: pwsh
        run: |
          docker volume inspect fraud-reference 2>$null
          if ($LASTEXITCODE -ne 0) {
          docker volume create fraud-reference
          }

      - name: Copy SQLite DB from persistent volume
        shell: pwsh
        run: |
          docker run --rm `
            -v fraud-data:/mnt `
            -v "$Env:GITHUB_WORKSPACE\data:/workspace/data" `
            alpine:latest `
            sh -c "cp /mnt/requests.db /workspace/data/requests.db"

      - name: Copy reference_data.db from persistent volume
        shell: pwsh
        run: |
          docker run --rm `
            -v fraud-reference:/mnt_ref `
            -v "$Env:GITHUB_WORKSPACE\data:/workspace/data" `
            alpine:latest `
            sh -c "cp /mnt_ref/reference_data.db /workspace/data/reference_data.db"

      - name: Upload reference-data artifact
        uses: actions/upload-artifact@v4
        with:
          name: reference-data
          path: data/reference_data.db


      - name: Upload latest-db artifact
        uses: actions/upload-artifact@v4
        with:
          name: latest-db
          path: data/requests.db

  # ───────────────────────────────────────────────────────────────
  # 2) drift_job: Download DBs and run data drift detection
  drift_job:
    needs: deploy_job
    runs-on: ubuntu-latest
    outputs:
      drift: ${{ steps.detect.outputs.drift }}
    steps:
      - uses: actions/checkout@v3

      - name: Download reference-data artifact
        uses: actions/download-artifact@v4
        with:
          name: reference-data
          path: data

      - name: Download latest-db artifact
        uses: actions/download-artifact@v4
        with:
          name: latest-db
          path: data

      - name: Install drift deps
        run: |
          sudo apt-get update
          sudo apt-get install -y jq
          pip install pandas scipy sqlalchemy

      - name: Run drift detection
        id: detect
        env:
          OLD_DATA_DB: data/reference_data.db
          NEW_DATA_DB: data/requests.db
        run: |
          python drift_detector.py
          DRIFT=$(jq .drift_detected drift_result.json)
          echo "drift=$DRIFT" >> $GITHUB_OUTPUT

      - name: Upload drift result
        uses: actions/upload-artifact@v4
        with:
          name: drift-result
          path: drift_result.json

  # ───────────────────────────────────────────────────────────────
  # 3) retrain_job: Retrain model if drift detected or on schedule
  retrain_job:
    needs: drift_job
    if: needs.drift_job.outputs.drift == 'true' || github.event_name == 'schedule'
    runs-on: ubuntu-latest

    services:
      pushgateway:
        image: prom/pushgateway:latest
        ports:
          - 9091:9091

    env:
      PUSHGATEWAY_URL: localhost:9091

    steps:
      - uses: actions/checkout@v3

      - name: Download latest-db artifact
        uses: actions/download-artifact@v4
        with:
          name: latest-db
          path: data

      - name: Install retrain deps
        run: pip install -r requirements.txt mlflow sqlalchemy

      - name: Retrain model on latest data
        env:
          TRAIN_DB: data/requests.db
        run: python retrain_model.py

      - name: Upload new model artifact
        uses: actions/upload-artifact@v4
        with:
          name: model-artifact
          path: models/rf_model.pkl
  # ───────────────────────────────────────────────────────────────
  # 4) update_reference: Overwrite reference volume after retraining
  update_reference:
    needs: retrain_job
    runs-on: [ self-hosted, Windows ]
    steps:
      - name: Ensure reference volume exists
        shell: pwsh
        run: |
          docker volume inspect fraud-reference -ErrorAction SilentlyContinue `
            || docker volume create fraud-reference

      - name: Update reference volume from latest data
        shell: pwsh
        run: |
          docker run --rm `
            -v fraud-data:/mnt_data `
            -v fraud-reference:/mnt_ref `
            alpine:latest `
            sh -c "cp /mnt_data/requests.db /mnt_ref/reference_data.db"

  # ───────────────────────────────────────────────────────────────
  # 5) build-and-push: Build Docker image and push to GHCR
  build-and-push:
    needs: retrain_job
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set lowercase owner
        shell: bash
        env:
          OWNER: ${{ github.repository_owner }}
        run: |
          OWNER_LC="${OWNER,,}"
          echo "OWNER_LC=$OWNER_LC" >> $GITHUB_ENV

      - name: Login to GHCR
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build & push Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: |
            ghcr.io/${{ env.OWNER_LC }}/fraud-api:latest
            ghcr.io/${{ env.OWNER_LC }}/fraud-api:${{ github.sha }}

  # ───────────────────────────────────────────────────────────────
  # 6) deploy_final: Deploy updated service locally
  deploy_final:
    needs: build-and-push
    runs-on: [ self-hosted, Windows ]
    env:
      API_TOKEN: ${{ secrets.API_TOKEN }}
    steps:
      - name: Pull & restart container locally
        shell: cmd
        run: |
          docker rm -f fraud_api || echo no container
          docker run -d --name fraud_api ^
            -v fraud-data:/app/data ^
            --env API_TOKEN=%API_TOKEN% ^
            --env FRAUD_THRESHOLD=0.4 ^
            --env LATEST_DB=/app/data/requests.db ^
            -p 5000:5000 ^
            ghcr.io/andipfluegl/fraud-api:latest